{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Visualization Notebook\n",
    "# Run each cell sequentially to load model and visualize predictions\n",
    "\n",
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project paths\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(project_root / \"preprocessing\"))\n",
    "sys.path.append(str(project_root / \"model_architecture\"))\n",
    "sys.path.append(str(project_root / \"metrics\"))\n",
    "sys.path.append(str(project_root / \"utils\"))\n",
    "sys.path.append(str(project_root / \"config\"))\n",
    "\n",
    "from dataloader import create_data_loaders, BurnSeverityDataset\n",
    "from visualization import visualize_model_predictions, visualize_single_prediction\n",
    "from unet import UNet\n",
    "from resUnet import ResUNet\n",
    "from attentionUnet import AttentionUNet\n",
    "from config import Config\n",
    "\n",
    "print(f\"Using device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model Loading Function\n",
    "def get_model(name: str, num_classes: int):\n",
    "    \"\"\"Factory to choose model by name.\"\"\"\n",
    "    if name.lower() == \"unet\":\n",
    "        return UNet(n_channels=Config.IN_CHANNELS, n_classes=num_classes)\n",
    "    elif name.lower() == \"resunet\":\n",
    "        return ResUNet(n_channels=Config.IN_CHANNELS, n_classes=num_classes)\n",
    "    elif name.lower() == \"attentionunet\":\n",
    "        return AttentionUNet(n_channels=Config.IN_CHANNELS, n_classes=num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {name}\")\n",
    "\n",
    "def load_trained_model(model_path: str, model_name: str, device: torch.device):\n",
    "    \"\"\"Load a trained model from checkpoint\"\"\"\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "    # Create model architecture\n",
    "    model = get_model(model_name, num_classes=Config.NUM_CLASSES)\n",
    "    \n",
    "    # Load trained weights\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    \n",
    "    # Move to device and set to eval mode\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Successfully loaded {model_name} model\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Your Specific Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your ResU-Net model\n",
    "model_path = \"logs/checkpoints/0.0002_v2_resunet_model.pth\" #-------------------------------------------LOAD MODEL HERE\n",
    "model_name = \"resunet\"\n",
    "\n",
    "model = load_trained_model(model_path, model_name, device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load Test Data\n",
    "print(\"Loading test data...\")\n",
    "_, _, test_loader = create_data_loaders(\n",
    "    dataset_path=Config.DATASET_PATH,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# Also create dataset object for single sample visualization\n",
    "test_dataset = BurnSeverityDataset(Config.DATASET_PATH, \"test\")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Visualize Model Predictions on Batch\n",
    "print(\"Visualizing model predictions on batch...\")\n",
    "visualize_model_predictions(\n",
    "    model, \n",
    "    test_loader, \n",
    "    device, \n",
    "    num_samples=4, \n",
    "    save_dir=\"results\", \n",
    "    prefix=\"resunet_predictions\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualize Single Sample Prediction (you can change the index)\n",
    "sample_idx = 26  # ------------------------------------------------------------ LOAD SINGLE SAMPLE HERE\n",
    "\n",
    "print(f\"Visualizing single prediction for sample {sample_idx}...\")\n",
    "metrics = visualize_single_prediction(\n",
    "    model, \n",
    "    test_dataset, \n",
    "    device, \n",
    "    idx=sample_idx, \n",
    "    save_dir=\"results\"\n",
    ")\n",
    "\n",
    "print(\"\\nSample metrics:\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.1f}%\")\n",
    "print(f\"Mean confidence: {metrics['mean_confidence']:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compare Multiple Single Samples\n",
    "print(\"Visualizing multiple single samples...\")\n",
    "sample_indices = [26,100,430]  # ALWAYS THESE SAMPLES TO COMPARE IN REPORT\n",
    "\n",
    "for idx in sample_indices:\n",
    "    if idx < len(test_dataset):\n",
    "        print(f\"\\nSample {idx}:\")\n",
    "        metrics = visualize_single_prediction(\n",
    "            model, \n",
    "            test_dataset, \n",
    "            device, \n",
    "            idx=idx, \n",
    "            save_dir=\"results\"\n",
    "        )\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.1f}%, Confidence: {metrics['mean_confidence']:.3f}\")\n",
    "    else:\n",
    "        print(f\"Index {idx} is out of range (max: {len(test_dataset)-1})\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "burn_severity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
